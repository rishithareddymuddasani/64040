---
title: "Assignment 5"
author: "Rishitha Reddy Muddasani"
date: "2023-04-15"
output:
  word_document: default
  html_document: default
  pdf_document: default
---



```{r}
#Displaying the required libraries
library(cluster)
library(caret)
library(dendextend)
library(knitr)
library(factoextra)
library(readr)
```

```{r}
#creating a data collection that solely includes numbers by importing a dataset
library(readr)
RM_Cereals <- read.csv("~/Downloads/Cereals.csv")
View(RM_Cereals)
Num_data <- data.frame(RM_Cereals[,4:16])
```

```{r}
#Missing values should be omitted
Num_data <- na.omit(Num_data)
```

```{r}
#Normalizing data
RM_Cereals_normalize <- scale(Num_data) #Data is normalized using scale function
```


***TASK 1***
```{r}
#Use the normalized data to do hierarchical clustering using the Euclidean Distance technique.
Dist <- dist(RM_Cereals_normalize, method = "euclidean")
# Hierarchical clustering using Complete Linkage
H_clust <- hclust(Dist, method = "complete")
```

```{r}
#the dendogram plotting process.
plot(H_clust, cex = 0.7, hang = -1) #Plots the obtained dendogram
#The dendogram helps us in determining the number of clusters required to classify this dataset.
```

```{r}
#Compute with AGNES and with different linkage methods
single_Hclust <- agnes(RM_Cereals_normalize, method = "single")
complete_Hclust <- agnes(RM_Cereals_normalize, method = "complete")
average_Hclust <- agnes(RM_Cereals_normalize, method = "average")
ward_Hclust <- agnes(RM_Cereals_normalize, method = "ward")
```

```{r}
#Choosing the most efficient course of action
print(single_Hclust$ac)
print(complete_Hclust$ac)
print(average_Hclust$ac)
print(ward_Hclust$ac)
```
#The ward strategy is the most successful one, as shown by its value of 0.9046042, which is evident given the facts provided.


***TASK 2***- How many clusters would you choose?
```{r}
#Using the Ward linkage, 5 clusters seem to be enough for grouping the data.
pltree(ward_Hclust, cex = 0.5, hang = -1, main = "Dendrogram of agnes (Using Ward)")
rect.hclust(ward_Hclust, k = 5, border = 2:7)
R_Group <- cutree(ward_Hclust, k=5)
D_frame_2 <- as.data.frame(cbind(RM_Cereals_normalize,R_Group))
```
```{r}
fviz_cluster(list(data = D_frame_2, cluster = R_Group))
```
# From the observation mentioned above, 5 clusters can be selected.


***TASK 3 ***- Determining the stability and structure of the clusters. 
```{r}
#Building Partitions: partition_one and partition_two
set.seed(123)
partition_one <- Num_data[1:55,]
partition_two <- Num_data[56:74,]
```

```{r}
#Performing Hierarchical Clustering while considering k = 5. Compute with AGNES and with different linkage methods for training dataset
single_rm <- agnes(scale(partition_one), method = "single")
complete_rm <- agnes(scale(partition_one), method = "complete")
average_rm <- agnes(scale(partition_one), method = "average")
ward_rm <- agnes(scale(partition_one), method = "ward")
cbind(single=single_rm$ac , complete=complete_rm$ac , average= average_rm $ac , ward= ward_rm$ac)
pltree(ward_rm, cex = 0.6, hang = -1, main = "Dendogram of Agnes with Partitioned Data (Using Ward)")
rect.hclust(ward_rm, k = 5, border = 2:7)
cut_2 <- cutree(ward_rm, k = 5)
```

```{r}
#the centroids are calculated.
RM_result <- as.data.frame(cbind(partition_one, cut_2))
RM_result[RM_result$cut_2==1,]
centroid_1 <- colMeans(RM_result[RM_result$cut_2==1,])
RM_result[RM_result$cut_2==2,]
centroid_2 <- colMeans(RM_result[RM_result$cut_2==2,])
RM_result[RM_result$cut_2==3,]
centroid_3 <- colMeans(RM_result[RM_result$cut_2==3,])
RM_result[RM_result$cut_2==4,]
centroid_4 <- colMeans(RM_result[RM_result$cut_2==4,])
centroids <- rbind(centroid_1, centroid_2, centroid_3, centroid_4)
x2 <- as.data.frame(rbind(centroids[,-14], partition_two))
```
```{r}
#figuring out the Distance.
Dist_1 <- get_dist(x2)
Matrix_1 <- as.matrix(Dist_1)
dataframe1 <- data.frame(data=seq(1,nrow(partition_two),1), Clusters = rep(0,nrow(partition_two)))
for(i in 1:nrow(partition_two)) 
  {dataframe1[i,2] <- which.min(Matrix_1[i+4, 1:4])}
dataframe1
cbind(D_frame_2$R_Group[56:74], dataframe1$Clusters)
table(D_frame_2$R_Group[56:74] == dataframe1$Clusters)
```
#From the above observation, we are getting 7 False and 12 True. Hence, we can conclude that the model is partially stable.


***TASK 4*** - The elementary public schools would like to choose a set of SB_Cereals to include in their daily cafeterias. Every day a different cereal is offered, but all SB_Cereals should support a healthy diet. For this goal, you are requested to find a cluster of â€œhealthy Cereals''

```{r}
#Clustering Healthy RM_Cereals.
Healthy_RM_Cereals <- RM_Cereals
Healthy_RM_Cereals_RD <- na.omit(Healthy_RM_Cereals)
clust <- cbind(Healthy_RM_Cereals_RD, R_Group)
clust[clust$R_Group==1,]
clust[clust$R_Group==2,]
clust[clust$R_Group==3,]
clust[clust$R_Group==4,]
```

```{r}
#Mean ratings are used to select the best cluster.
mean(clust[clust$R_Group==1,"rating"])
mean(clust[clust$R_Group==2,"rating"])
mean(clust[clust$R_Group==3,"rating"])
mean(clust[clust$R_Group==4,"rating"])
```

#Cluster 1 may be chosen based on the data mentioned above because it is the highest.
#Therefore, Group 1 may be considered of as the cluster for a healthy diet.